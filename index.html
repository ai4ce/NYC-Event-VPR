<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Event-based Visual Place Recognition Dataset">
  <meta name="keywords" content="VPR, Event Camera">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    table, th, td {
      border: 1px solid black;
      border-collapse: collapse;
    }
  </style>
  <title>NYC-Event-VPR</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->
  <link rel="icon" type="image/png" href="./static/images/ai4ce.png"> 
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/airlab.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://ai4ce.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">NYC-Event-VPR&nbsp;&nbsp;</h1>
          <h1 class="title is-2 publication-title">A Large-Scale High-Resolution Event-Based Visual Place Recognition Dataset in Dense Urban Environments</h1>
          <div class="column is-full_width">
            <h2 class="title is-4">Submitted to <a href="https://2024.ieee-icra.org/index.html">ICRA 2024</a></h2>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://www.taiyipan.org/">Taiyi Pan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://joechencc.github.io">Chao Chen</a><sup>1</sup></span><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=i_aajNoAAAAJ">Yiming Li</a><sup>1</sup><sup>*</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ">Chen Feng</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>New York University, Brooklyn, NY 11201, USA</span>
          </div>
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contributions</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2208.09315"                    
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2208.09315"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
               <a href=""                    
                  class="external-link button is-normal is-rounded is-dark">
                 <span class="icon">
                     <i class="fas fa-camera"></i>
                 </span>
                 <span>Appendix</span>
               </a>
             </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ai4ce/NYC-Event-VPR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ai4ce/NYC-Event-VPR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" controls loop height="100%">
        <source src="./static/video/Multimedia.mp4"
                type="video/mp4">
      </video> -->
      <iframe width="640" height="480" src="https://www.youtube.com/embed/hB8DkfqB15I" frameborder="0"></iframe>
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <br><br><br>
      <h2 class="subtitle has-text-centered">
      Dataset visualization
    </h2>
    <!-- <h2 class="subtitle has-text-centered">
      (The <span style="color:#000000;">black</span> / <span style="color:#ff0000;">red</span> lines are the ground truth / predicted camera trajectory)
    </h2> -->    
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop height="100%">
        <source src="./static/images/SUBT_video.mp4"
                type="video/mp4">
      </video> -->
      <img src="./static/images/front_page_showcase.jpg" height="300">
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <br><br><br>
      <h2 class="subtitle has-text-centered">
        Downtown Manhattan in event frame, reconstructed frame, and RGB frame.
    </h2>
    
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p><strong>Visual place recognition (VPR)</strong> enables autonomous robots to identify previously visited locations, which
            contributes to tasks like simultaneous localization and mapping
            (SLAM). VPR faces challenges such as accurate image neighbor
            retrieval and appearance change in scenery.</p>
          <p><strong>Event cameras</strong>, also
            known as dynamic vision sensors, are a new sensor modality
            for VPR and offer a promising solution to the challenges
            with their unique attributes: high temporal resolution (1MHz
            clock), ultra-low latency (in μs), and high dynamic range
            (>120dB). These attributes make event cameras less susceptible
            to motion blur and more robust in variable lighting conditions,
            making them suitable for addressing VPR challenges. However,
            the scarcity of event-based VPR datasets, partly due to the
            novelty and cost of event cameras, hampers their adoption.</p>
          <p>To fill this data gap, our paper introduces the <strong>NYC-Event-VPR dataset</strong> to the robotics and computer vision communities,
            featuring the Prophesee IMX636 HD event sensor (1280x720
            resolution), combined with RGB camera and GPS module. It
            encompasses over 13 hours of geotagged event data, spanning
            260+ kilometers across New York City, covering diverse lighting
            and weather conditions, day/night scenarios, and multiple visits
            to various locations.</p>
          <p>Furthermore, our paper employs the
            VPR-Bench framework to conduct generalization performance
            assessments, promoting innovation in event-based VPR and its
            integration into robotics applications.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full_width">
        <hr>
        <h2 class="title is-3">Dataset</h2>
        <br>
        <table style="width:100%">
          <tr>
            <th>Duration (hr)</th><th>Data size (GB)</th><th>Modality</th><th>Distance (km)</th><th>Weather</th><th>Lighting conditions</th><th>Resolution (px)</th>
          </tr>
          <tr>
            <td>13.5</td><td>466.7</td><td>event, RGB, GPS</td><td>259.95</td><td>rainy, cloudy, sunny</td><td>day, night</td><td>1280 x 720</td>
          </tr>
        </table>
        <div class="content has-text-centered">
          <br>  
          <p>
            NYC-Event-VPR dataset statistics.
          </p>
        </div>
        <br>
        <img src="./static/images/total_gps_coverage.png" style="object-fit: contain; height: 600px;" class="center"/>
        <div class="content has-text-justified">
          <br>  
          <p>
            NYC-Event-VPR covers New York City, focusing on Chinatown area in Manhattan with overlapping traversal.
          </p>
        </div>
        <br>
        <img src="./static/images/sensor_setup.jpg" style="object-fit: contain; height: 200px;" class="center"/>
        <div class="content has-text-justified">
          <br>  
          <p>
            Sensor setup and mounting design: RGB camera is mounted on top of event camera, and the sensor suite is positioned facing forward behind vehicle`s front windshield.
          </p>
        </div>
        <br>
        <table style="width:100%" class="center">
          <tr>
            <th>Type</th><th>Specification</th>
          </tr>
          <tr>
            <td>Prophesee EV4 HD</td><td>
              <pre>
                IMX636ES (HD) event vision sensor,
                Resolution (px): 1280x720,
                Latency (µs): 220,
                Dynamic range (dB): >86,
                Power consumption: 500mW-1.5W,
                Pixel size (µm): 4.86x4.86,
                Camera max bandwidth (Gbps): 1.6,
                Interface: USB 3.0
              </pre>
            </td>
          </tr>
          <tr>
            <td>ELP USB Camera</td><td>
              <pre>
                CMOS 1080p sensor,
                Resolution (px): 1280x720,
                Interface: USB 2.0,
                5-50mm varifocal lens
              </pre>
            </td>
          </tr>
          <tr>
            <td>Sparkfun GPS-RTK-SMA</td><td>
              <pre>
                Horizontal accuracy: 2.5m w/o RTK,
                Max altitude: 50km,
                Max velocity: 500m/s,
                GPS, GLONASS, Galileo, BeiDou
              </pre>
            </td>
          </tr>
        </table>
        <div class="content has-text-centered">
          <br>  
          <p>
            Sensor specifications.
          </p>
        </div>
        <br>
        <img src="./static/images/report.png" style="object-fit: contain; height: 500px;" class="center"/>
        <div class="content has-text-justified">
          <br>  
          <p>
            Example images in processed dataset (from left to right columns): naive conversion, E2VID reconstruction, RGB reference. Each row is the same visual scene. Each column is the same dataset.
          </p>
        </div>
      </div>
    </div>
    <hr>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full_width">
        <hr>
        <h2 class="title is-3">Benchmark</h2>
        <br>
        <table style="width:100%">
          <tr><th>Datasets</th><th>NetVLAD</th><th>RegionVLAD</th><th>HOG</th><th>AMOSNet</th><th>HybridNet</th><th>CALC</th></tr>
          <tr><td>NYC-Event-VPR-Naive-5m</td><td>32.53</td><td>24.85</td><td>25.13</td><td>65.76</td><td>68.69</td><td>74.17</td></tr>
          <tr><td>NYC-Event-VPR-Naive-2m</td><td>25.68</td><td>20.35</td><td>21.02</td><td>57.74</td><td>61.84</td><td>66.87</td></tr>
          <tr><td>NYC-Event-VPR-E2VID-5m</td><td>74.06</td><td>77.56</td><td>86.03</td><td>86.22</td><td>85.51</td><td>84.69</td></tr>
          <tr><td>NYC-Event-VPR-E2VID-2m</td><td>68.08</td><td>73.44</td><td>83.84</td><td>82.99</td><td>82.12</td><td>81.05</td></tr>
          <tr><td>NYC-Event-VPR-RGB-5m</td><td>92.52</td><td>92.58</td><td>92.43</td><td>94.52</td><td>94.33</td><td>92.86</td></tr>
          <tr><td>NYC-Event-VPR-RGB-2m</td><td>88.34</td><td>89.94</td><td>91.48</td><td>91.82</td><td>92.36</td><td>90.16</td></tr>
          <tr><td>Pittsburgh250K</td><td>94.36</td><td>73.34</td><td>0.27</td><td>8.53</td><td>8.70</td><td>2.05</td></tr>
          <tr><td>Nordland</td><td>8.49</td><td>13.33</td><td>2.89</td><td>30.13</td><td>17.52</td><td>12.91</td></tr>
        </table>
        <div class="content has-text-justified">
          <br>  
          <p>
            Quantitative results of Area Under the Curve- Precision Recall (AUC-PR) in percentage.
          </p>
          <p>
            Top 6 datasets are from NYC-Event-VPR. Each row represents a set of 6 benchmark tests on that dataset. Bottom 2 datasets are curated subsets of their respective datasets provided by VPR-Bench. All tests are done using pretrained weights provided by <a href="https://github.com/MubarizZaffar/VPR-Bench">VPR-Bench</a>.
          </p>
        </div>
        <br>
        <table style="width:100%">
          <tr><th>CCT384+NetVLAD</th><th>Naive-5m</th><th>E2VID-5m</th><th>RGB-5m</th><th>Naive-15m</th><th>E2VID-15m</th><th>RGB-15m</th><th>Naive-25m</th><th>E2VID-25m</th><th>RGB-25m</th></tr>
          <tr><th>Recall@1</th><td>33.2</td><td>48.7</td><td>57.7</td><td>51</td><td>70.5</td><td>77.8</td><td>57.3</td><td>77.9</td><td>86</td></tr>
          <tr><th>Recall@5</th><td>45.1</td><td>54.4</td><td>61.3</td><td>64</td><td>78.2</td><td>81.4</td><td>74.1</td><td>85.4</td><td>90.1</td></tr>
          <tr><th>Recall@10</th><td>48.7</td><td>55.8</td><td>62.4</td><td>68.6</td><td>80.3</td><td>82.4</td><td>79.5</td><td>87.1</td><td>91</td></tr>
          <tr><th>Recall@20</th><td>51.2</td><td>57.1</td><td>63.2</td><td>73.2</td><td>81.7</td><td>83.1</td><td>83.9</td><td>88.7</td><td>91.7</td></tr>
        </table>
        <br>
        <table style="width:100%">
          <tr><th>ResNet50+NetVLAD</th><th>Naive-5m</th><th>E2VID-5m</th><th>RGB-5m</th><th>Naive-15m</th><th>E2VID-15m</th><th>RGB-15m</th><th>Naive-25m</th><th>E2VID-25m</th><th>RGB-25m</th></tr>
          <tr><th>Recall@1</th><td>39.1</td><td>51.9</td><td>59.1</td><td>54.9</td><td>73.1</td><td>79</td><td>62.3</td><td>80.5</td><td>86.9</td></tr>
          <tr><th>Recall@5</th><td>48.9</td><td>54.7</td><td>62.2</td><td>68.6</td><td>78.4</td><td>82.4</td><td>76.4</td><td>85.8</td><td>91</td></tr>
          <tr><th>Recall@10</th><td>51.5</td><td>55.6</td><td>62.5</td><td>72.6</td><td>79.8</td><td>83.1</td><td>80.7</td><td>87.5</td><td>91.5</td></tr>
          <tr><th>Recall@20</th><td>53.1</td><td>56.2</td><td>62.8</td><td>75.8</td><td>80.9</td><td>83.5</td><td>83.7</td><td>89</td><td>92</td></tr>
        </table>
        <div class="content has-text-justified">
          <br>  
          <p>
            Quantitative results of recall@k in percentage.
          </p>
          <p>
            First table is test results of deep learning model trained on NYC-Event-VPR. Backbone is CCT384 (<a href="https://arxiv.org/abs/2104.05704">Compact Convolutional Transformer</a>). Aggregation is done via <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.html">NetVLAD</a>. 
          </p>
          <p>
            Second table is also test results of deep learning model trained on NYC-Event-VPR. Backbone is ResNet50 (<a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Residual Network</a>). Aggregation is done via <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.html">NetVLAD</a>.  
          </p>
          <p>
             All benchmarks are done by training the model on NYC-Event-VPR dataset using <a href="https://github.com/gmberton/deep-visual-geo-localization-benchmark">Deep Visual Geo-localization Benchmark</a> framework.
          </p>
        </div>
      </div>
    </div>
    <hr>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{Pan2024arxiv,
      author    = {Pan, Taiyi and Chen, Chao and Li, Yiming and Feng, Chen},
      title     = {NYC-Event-VPR: A Large-Scale High-Resolution Event-Based Visual Place Recognition Dataset in Dense Urban Environments},
      journal = {arXiv:2208.09315},
      year      = {2024}
  }</code></pre>
  </div>
</section> -->

<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    Chen Feng is the corresponding author (cfeng@nyu.edu). This work is supported by NSF Grant 2238968.
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
